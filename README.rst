######################################
Apache Hive over Object Storage System 
######################################

Easy way to prepare infrastructure to demonstrate Hadoop's support for object storage systems using the ``s3a`` protocol.  More information about ``s3a`` can be found `here. <https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html>`_

Object storage system provided by `MINIO. <https://min.io/>`_

Notebook used is `Zeppelin. <https://zeppelin.apache.org/>`_

*************
Prerequisites
*************

* `Docker <https://docs.docker.com/install/>`_

***************
Getting Started
***************

Get the code::

    $ git clone https://github.com/loum/hadoop-object-store && cd hadoop-object-store

.. note::

    Run all commands from the top-level directory of the `git` repository.

For first-time setup, get the `Makester project <https://github.com/loum/makester.git>`_::

    $ git submodule update --init

Keep `Makester project <https://github.com/loum/makester.git>`_ up-to-date with::

    $ git submodule update --remote --merge

************
Getting Help
************

There should be a ``make`` target to be able to get most things done.  Check the help for more information::

    $ make help

********************
Development Pipeline
********************

Setup the environment::

    $ make init

The following example demonstrates how we can set up a data lake over and object storage storage.  First, bring up the infrastructure::

    $ make local-build-up

For demonstration purposes and to avoid AWS interfaces during PoC, we will be using MINIO as the data lake.  Navigate to `<http://127.0.0.1:9000>`_ and login with the hardwired test credentials.

.. note::

    The following are TEST credentials only that were auto-generated by the MINIO docker container on initial start up and re-used here for simplicity.  **Do not use these credentials in a production environment.**

**Access Key:** ``05Y2TVZ3T1RQNH7TI89Q``

**Secret Key:** ``8P2AajiFu+CHo2+3M2pUgWBhtVLaYUXBqBjGZ3wP``

Once logged in you should see the ``hive-warehouse`` bucket.

Cleanup
=======

Remove the containers and data::

    $ make local-build-down
